mod resource_state;
mod worker;
mod allocator;
mod recorder;

use std::collections::{VecDeque};
use std::panic::{RefUnwindSafe, UnwindSafe};
use std::sync::{Arc, Condvar, Mutex, Weak};
use std::thread::JoinHandle;

use ash::vk;

use crate::prelude::*;
use crate::device::device::Queue;
use crate::vk::objects::allocator::Allocator;
use crate::vk::objects::buffer::Buffer;

use worker::*;
use crate::objects::id::{BufferId, ImageId, ObjectId};
use crate::objects::sync::{SemaphoreOp, SemaphoreOps};
use crate::vk::objects::image::Image;

#[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Ord, Debug, Hash)]
pub enum AcquireError {
    AlreadyAvailable,
}

#[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Ord, Debug, Hash)]
pub enum ReleaseError {
    NotAvailable,
}

#[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Ord, Debug, Hash)]
pub struct SyncId(u64);

impl SyncId {
    pub fn from_raw(raw: u64) -> Self {
        Self(raw)
    }

    pub fn get_raw(&self) -> u64 {
        self.0
    }
}

pub struct Transfer {
    weak: Weak<Self>,
    share: Arc<Share>,
    queue_family: u32,
    worker: Option<JoinHandle<()>>,
}

// TODO i hate this
impl UnwindSafe for Transfer {
}
impl RefUnwindSafe for Transfer {
}

impl Transfer {
    pub fn new(device: Arc<DeviceFunctions>, alloc: Arc<Allocator>, queue: Arc<Queue>) -> Arc<Self> {
        let queue_family = queue.get_queue_family_index();
        log::debug!("Creating transfer engine on queue family {:?}", queue_family);

        let share = Arc::new(Share::new(device, alloc));

        let share2 = share.clone();
        let worker = std::thread::spawn(move || {
            log::debug!("Starting transfer worker on queue family {:?}", queue_family);
            std::panic::catch_unwind(|| {
                run_worker(share2, queue);
            }).unwrap_or_else(|r| {
                log::error!("Transfer worker panicked: {:?}", r);
                std::process::exit(1);
            })
        });

        Arc::new_cyclic(|weak| Self {
            weak: weak.clone(),
            share,
            queue_family,
            worker: Some(worker)
        })
    }

    /// Returns the queue family index of the queue that is used for transfer operations.
    pub fn get_queue_family(&self) -> u32 {
        self.queue_family
    }

    /// Generates a buffer acquire operation for some buffer.
    ///
    /// This does **not** make the buffer available. It just collects information about the buffer
    /// and generates a potential memory barrier so that the calling code may submit it.
    ///
    /// - If `usage` is [`None`] no memory barrier will be generated.
    /// - If `usage` is [`Some`] the contents should be the source stage mask, source access mask
    /// and the source queue family index needed for a potential barrier. This function will
    /// determine if a memory barrier is necessary and if so generate one.
    pub fn prepare_buffer_acquire(&self, buffer: Buffer, usage: Option<(vk::PipelineStageFlags2, vk::AccessFlags2, u32)>) -> BufferAcquireOp {
        if let Some((src_stage_mask, src_access_mask, src_queue_family)) = usage {
            let queue_info =
                if src_queue_family == self.queue_family {
                    None
                } else {
                    Some((src_queue_family, self.queue_family))
                };

            BufferAcquireOp {
                buffer,
                offset: 0,
                size: vk::WHOLE_SIZE,
                src_info: Some((src_stage_mask, src_access_mask)),
                queue_info,
            }
        } else {
            BufferAcquireOp {
                buffer,
                offset: 0,
                size: vk::WHOLE_SIZE,
                src_info: None,
                queue_info: None
            }
        }
    }

    /// Makes a buffer available for transfer operations.
    ///
    /// The `op` can be generated by a call to [`prepare_buffer_acquire`]. If that call has
    /// generated a memory barrier the calling code **must** submit that barrier before calling this
    /// function.
    ///
    /// A list of wait semaphores can be provided through `semaphores`. All provided semaphores must
    /// have been submitted before this function is called.
    pub fn acquire_buffer(&self, op: BufferAcquireOp, semaphores: SemaphoreOps) -> Result<(), AcquireError> {
        self.share.push_task(Task::BufferAcquire(op, semaphores));
        Ok(())
    }

    /// Generates a buffer release operation for some buffer.
    ///
    /// This does **not** release the buffer. It just collects information about the buffer and
    /// generates a potential memory barrier.
    ///
    /// - If `usage` is [`None`] no memory barrier will be generated.
    /// - If `usage` is [`Some`] the contents should be the destination stage mask, destination
    /// access mask and the destination queue family index needed for a potential barrier. This
    /// function will determine if a memory barrier is necessary and if so generate one.
    pub fn prepare_buffer_release(&self, buffer: Buffer, usage: Option<(vk::PipelineStageFlags2, vk::AccessFlags2, u32)>) -> BufferReleaseOp {
        if let Some((dst_stage_mask, dst_access_mask, dst_queue_family)) = usage {
            let queue_info =
                if dst_queue_family == self.queue_family {
                    None
                } else {
                    Some((self.queue_family, dst_queue_family))
                };

            BufferReleaseOp {
                buffer,
                offset: 0,
                size: vk::WHOLE_SIZE,
                dst_info: Some((dst_stage_mask, dst_access_mask)),
                queue_info,
            }
        } else {
            BufferReleaseOp {
                buffer,
                offset: 0,
                size: vk::WHOLE_SIZE,
                dst_info: None,
                queue_info: None
            }
        }
    }

    /// Revokes availability of a buffer from transfer operations.
    ///
    /// The `op` can be generated by a call to [`prepare_buffer_release`]. If that call has
    /// generated a memory barrier the calling code **must** submit that barrier before using the
    /// buffer.
    ///
    /// Submission may happen asynchronously. As such the calling code must to call
    /// [`wait_for_submit`] with the returned id before submitting command buffers with a potential
    /// acquire barrier.
    ///
    /// A wait semaphore for future submissions can be generated by calling
    /// [`generate_wait_semaphore`] with the returned id.
    pub fn release_buffer(&self, op: BufferReleaseOp) -> Result<SyncId, ReleaseError> {
        let id = self.share.push_buffer_release_task(op);
        Ok(SyncId::from_raw(id))
    }

    /// Generates a image acquire operation for some image.
    ///
    /// This does **not** make the image available. It just collects information about the image
    /// and generates a potential memory barrier so that the calling code may submit it.
    ///
    /// - If `usage` is [`None`] no memory barrier will be generated. Initial layout will be assumed
    /// to be [`vk::ImageLayout::UNDEFINED`].
    /// - If `usage` is [`Some`] the contents should be the source stage mask, source access mask,
    /// the source queue family index and the source image layout. This function will determine if a
    /// memory barrier is necessary and if so generate one.
    pub fn prepare_image_acquire(&self, image: Image, aspect_mask: vk::ImageAspectFlags, usage: Option<(vk::PipelineStageFlags2, vk::AccessFlags2, u32, vk::ImageLayout)>) -> ImageAcquireOp {
        if let Some((src_stage_mask, src_access_mask, src_queue_family, src_layout)) = usage {
            let queue_info =
                if src_queue_family == self.queue_family {
                    None
                } else {
                    Some((src_queue_family, src_queue_family))
                };

            ImageAcquireOp {
                image,
                subresource_range: vk::ImageSubresourceRange {
                    aspect_mask,
                    base_mip_level: 0,
                    level_count: vk::REMAINING_MIP_LEVELS,
                    base_array_layer: 0,
                    layer_count: vk::REMAINING_ARRAY_LAYERS
                },
                src_info: Some((src_stage_mask, src_access_mask, src_layout)),
                queue_info
            }
        } else {
            ImageAcquireOp {
                image,
                subresource_range: vk::ImageSubresourceRange {
                    aspect_mask,
                    base_mip_level: 0,
                    level_count: vk::REMAINING_MIP_LEVELS,
                    base_array_layer: 0,
                    layer_count: vk::REMAINING_ARRAY_LAYERS,
                },
                src_info: None,
                queue_info: None
            }
        }
    }

    /// Makes a image available for transfer operations.
    ///
    /// The `op` can be generated by a call to [`prepare_image_acquire`]. If that call has
    /// generated a memory barrier the calling code **must** submit that barrier before calling this
    /// function.
    ///
    /// A list of wait semaphores can be provided through `semaphores`. All provided semaphores must
    /// have been submitted before this function is called.
    pub fn make_image_available(&self, op: ImageAcquireOp, semaphores: SemaphoreOps) -> Result<(), AcquireError> {
        self.share.push_task(Task::ImageAcquire(op, semaphores));
        Ok(())
    }

    /// Generates a image release operation for some image.
    ///
    /// This does **not** release the image. It just collects information about the image and
    /// generates a potential memory barrier.
    ///
    /// - If `usage` is [`None`] no memory barrier will be generated.
    /// - If `usage` is [`Some`] the contents should be the destination stage mask, destination
    /// access maks, destination queue family and destination image layout. This function will
    /// determine if a memory barrier is necessary and if so generate one.
    pub fn prepare_image_release(&self, image: Image, aspect_mask: vk::ImageAspectFlags, usage: Option<(vk::PipelineStageFlags2, vk::AccessFlags2, u32, vk::ImageLayout)>) -> ImageReleaseOp {
        if let Some((dst_stage_mask, dst_access_mask, dst_queue_family, dst_layout)) = usage {
            let queue_info =
                if dst_queue_family == self.queue_family {
                    None
                } else {
                    Some((self.queue_family, dst_queue_family))
                };

            ImageReleaseOp {
                image,
                subresource_range: vk::ImageSubresourceRange {
                    aspect_mask,
                    base_mip_level: 0,
                    level_count: vk::REMAINING_MIP_LEVELS,
                    base_array_layer: 0,
                    layer_count: vk::REMAINING_ARRAY_LAYERS,
                },
                dst_info: Some((dst_stage_mask, dst_access_mask, dst_layout)),
                queue_info
            }
        } else {
            ImageReleaseOp {
                image,
                subresource_range: Default::default(),
                dst_info: None,
                queue_info: None
            }
        }
    }

    /// Revokes availability of a image from transfer operations.
    ///
    /// The `op` can be generated by a call to [`prepare_image_release`]. If that call has
    /// generated a memory barrier the calling code **must** submit that barrier before using the
    /// image.
    ///
    /// Submission may happen asynchronously. As such the calling code must to call
    /// [`wait_for_submit`] with the returned id before submitting command buffers with a potential
    /// acquire barrier.
    ///
    /// A wait semaphore for future submissions can be generated by calling
    /// [`generate_wait_semaphore`] with the returned id.
    pub fn release_image(&self, op: ImageReleaseOp) -> Result<SyncId, ReleaseError> {
        let id = self.share.push_image_release_task(op);
        Ok(SyncId::from_raw(id))
    }

    /// Returns some staging memory which can be used to upload to or download data from the device.
    ///
    /// The returned memory is at least as large as `capacity` but may be larger.
    pub fn request_staging_memory(&self, capacity: usize) -> StagingMemory {
        let (id, alloc) = self.share.allocate_staging(capacity as vk::DeviceSize);

        StagingMemory {
            transfer: self.weak.upgrade().unwrap(),
            memory: alloc.get_memory().as_ptr(),
            memory_size: alloc.get_size() as usize,
            memory_id: id,
            buffer_offset: alloc.get_offset()
        }
    }

    pub fn flush(&self, id: SyncId) {
        self.share.push_task(Task::Flush(id.get_raw()));
    }

    pub fn wait_for_submit(&self, id: SyncId) {
        self.flush(id);
        self.share.wait_for_submit(id.get_raw());
    }

    pub fn wait_for_complete(&self, id: SyncId) {
        self.flush(id);
        self.share.wait_for_complete(id.get_raw());
    }

    pub fn generate_wait_semaphore(&self, id: SyncId) -> SemaphoreOp {
        self.share.get_sync_wait_op(id.get_raw())
    }
}

impl Drop for Transfer {
    fn drop(&mut self) {
        self.share.terminate();
        if let Some(worker) = self.worker.take() {
            match worker.join() {
                Err(_) => {
                    log::error!("Transfer channel worker panicked!");
                },
                _ => {}
            }
        } else {
            log::error!("Transfer channel worker join handle has been taken before drop!");
        }
    }
}

#[derive(Copy, Clone, Debug)]
pub struct BufferAcquireOp {
    buffer: Buffer,
    offset: vk::DeviceSize,
    size: vk::DeviceSize,
    src_info: Option<(vk::PipelineStageFlags2, vk::AccessFlags2)>,
    queue_info: Option<(u32, u32)>,
}

impl BufferAcquireOp {
    /// Returns a barrier which needs to be submitted by the user of the transfer engine before
    /// calling [`Transfer::acquire_buffer`]. If [`None`] is returned no barrier needs to be
    /// submitted.
    pub fn make_barrier(&self) -> Option<vk::BufferMemoryBarrier2> {
        // We only generate a user barrier if a queue family transfer is necessary
        self.queue_info.as_ref().map(|(src_queue_family, dst_queue_family)| {
            let (src_stage_mask, src_access_mask) = self.src_info.as_ref().unwrap();

            vk::BufferMemoryBarrier2::builder()
                .buffer(self.buffer.get_handle())
                .offset(self.offset)
                .size(self.size)
                .src_stage_mask(*src_stage_mask)
                .src_access_mask(*src_access_mask)
                .src_queue_family_index(*src_queue_family)
                .dst_queue_family_index(*dst_queue_family)
                .build()
        })
    }

    /// Returns the buffer used in this op
    pub fn get_buffer(&self) -> Buffer {
        self.buffer
    }

    /// Returns a barrier which needs to be submitted by the transfer engine before using the buffer.
    fn make_transfer_barrier(&self, dst_stage_mask: vk::PipelineStageFlags2, dst_access_mask: vk::AccessFlags2) -> Option<vk::BufferMemoryBarrier2> {
        self.src_info.as_ref().map(|(src_stage_mask, src_access_mask)| {
            let mut barrier = vk::BufferMemoryBarrier2::builder()
                .buffer(self.buffer.get_handle())
                .offset(self.offset)
                .size(self.size)
                .dst_stage_mask(dst_stage_mask)
                .dst_access_mask(dst_access_mask);

            if let Some((src_queue_family, dst_queue_family)) = &self.queue_info {
                barrier = barrier
                    .src_queue_family_index(*src_queue_family)
                    .dst_queue_family_index(*dst_queue_family);
            } else {
                barrier = barrier
                    .src_stage_mask(*src_stage_mask)
                    .src_access_mask(*src_access_mask);
            }

            barrier.build()
        })
    }
}

#[derive(Copy, Clone, Debug)]
pub struct BufferReleaseOp {
    buffer: Buffer,
    offset: vk::DeviceSize,
    size: vk::DeviceSize,
    dst_info: Option<(vk::PipelineStageFlags2, vk::AccessFlags2)>,
    queue_info: Option<(u32, u32)>,
}

impl BufferReleaseOp {
    /// Returns a barrier which needs to be submitted by the user of the transfer engine after
    /// calling [`Transfer::release_buffer`]. If [`None`] is returned no barrier needs to be
    /// submitted.
    ///
    /// Note that vulkan requires a potential queue family acquire barrier to be submitted after
    /// its corresponding release barrier. Since operations in the transfer engine are submitted
    /// asynchronously the user may need to call [`Transfer::wait_for_submit`] before submitting
    /// this barrier.
    pub fn make_barrier(&self) -> Option<vk::BufferMemoryBarrier2> {
        // We only generate a user barrier if a queue family transfer is necessary
        self.queue_info.as_ref().map(|(src_queue_family, dst_queue_family)| {
            let (dst_stage_mask, dst_access_mask) = self.dst_info.as_ref().unwrap();

            vk::BufferMemoryBarrier2::builder()
                .buffer(self.buffer.get_handle())
                .offset(self.offset)
                .size(self.size)
                .dst_stage_mask(*dst_stage_mask)
                .dst_access_mask(*dst_access_mask)
                .src_queue_family_index(*src_queue_family)
                .dst_queue_family_index(*dst_queue_family)
                .build()
        })
    }

    /// Returns the buffer used in this op
    pub fn get_buffer(&self) -> Buffer {
        self.buffer
    }

    /// Returns a barrier which needs to be submitted by the transfer engine before using the buffer.
    fn make_transfer_barrier(&self, src_stage_mask: vk::PipelineStageFlags2, src_access_mask: vk::AccessFlags2) -> Option<vk::BufferMemoryBarrier2> {
        self.dst_info.as_ref().map(|(dst_stage_mask, dst_access_mask)| {
            let mut barrier = vk::BufferMemoryBarrier2::builder()
                .buffer(self.buffer.get_handle())
                .offset(self.offset)
                .size(self.size)
                .src_stage_mask(src_stage_mask)
                .src_access_mask(src_access_mask);

            if let Some((src_queue_family, dst_queue_family)) = &self.queue_info {
                barrier = barrier
                    .src_queue_family_index(*src_queue_family)
                    .dst_queue_family_index(*dst_queue_family);
            } else {
                barrier = barrier
                    .dst_stage_mask(*dst_stage_mask)
                    .dst_access_mask(*dst_access_mask);
            }

            barrier.build()
        })
    }
}

#[derive(Copy, Clone, Debug)]
pub struct ImageAcquireOp {
    image: Image,
    subresource_range: vk::ImageSubresourceRange,
    src_info: Option<(vk::PipelineStageFlags2, vk::AccessFlags2, vk::ImageLayout)>,
    queue_info: Option<(u32, u32)>,
}

impl ImageAcquireOp {
    pub fn make_barrier(&self) -> Option<vk::ImageMemoryBarrier2> {
        // We only need a barrier on the user side if a queue family transfer is necessary
        self.queue_info.as_ref().map(|(src_queue_family, dst_queue_family)| {
            let (src_stage_mask, src_access_mask, src_layout) = self.src_info.unwrap();

            vk::ImageMemoryBarrier2::builder()
                .src_stage_mask(src_stage_mask)
                .src_access_mask(src_access_mask)
                .old_layout(src_layout)
                .new_layout(vk::ImageLayout::GENERAL)
                .src_queue_family_index(*src_queue_family)
                .dst_queue_family_index(*dst_queue_family)
                .image(self.image.get_handle())
                .subresource_range(self.subresource_range)
                .build()
        })
    }

    /// Returns the image used in this op
    pub fn get_image(&self) -> Image {
        self.image
    }

    /// Returns a barrier which needs to be submitted by the transfer engine before using the image.
    fn make_transfer_barrier(&self, dst_stage_mask: vk::PipelineStageFlags2, dst_access_mask: vk::AccessFlags2) -> Option<vk::ImageMemoryBarrier2> {
        self.src_info.as_ref().map(|(src_stage_mask, src_access_mask, src_layout)| {
            let mut barrier = vk::ImageMemoryBarrier2::builder()
                .image(self.image.get_handle())
                .subresource_range(self.subresource_range)
                .dst_stage_mask(dst_stage_mask)
                .dst_access_mask(dst_access_mask)
                .old_layout(*src_layout)
                .new_layout(vk::ImageLayout::GENERAL);

            if let Some((src_queue_family, dst_queue_family)) = &self.queue_info {
                barrier = barrier
                    .src_queue_family_index(*src_queue_family)
                    .dst_queue_family_index(*dst_queue_family);
            } else {
                barrier = barrier
                    .src_stage_mask(*src_stage_mask)
                    .src_access_mask(*src_access_mask);
            }

            barrier.build()
        })
    }
}

#[derive(Copy, Clone, Debug)]
pub struct ImageReleaseOp {
    image: Image,
    subresource_range: vk::ImageSubresourceRange,
    dst_info: Option<(vk::PipelineStageFlags2, vk::AccessFlags2, vk::ImageLayout)>,
    queue_info: Option<(u32, u32)>,
}

impl ImageReleaseOp {
    pub fn make_barrier(&self) -> Option<vk::ImageMemoryBarrier2> {
        // We only need a barrier on the user side if a queue family transfer is necessary
        self.queue_info.as_ref().map(|(src_queue_family, dst_queue_family)| {
            let (dst_stage_mask, dst_access_mask, dst_layout) = self.dst_info.unwrap();

            vk::ImageMemoryBarrier2::builder()
                .dst_stage_mask(dst_stage_mask)
                .dst_access_mask(dst_access_mask)
                .old_layout(vk::ImageLayout::GENERAL)
                .new_layout(dst_layout)
                .src_queue_family_index(*src_queue_family)
                .dst_queue_family_index(*dst_queue_family)
                .image(self.image.get_handle())
                .subresource_range(self.subresource_range)
                .build()
        })
    }

    /// Returns the image used in this op
    pub fn get_image(&self) -> Image {
        self.image
    }

    /// Returns a barrier which needs to be submitted by the transfer engine before using the image.
    fn make_transfer_barrier(&self, src_stage_mask: vk::PipelineStageFlags2, src_access_mask: vk::AccessFlags2) -> Option<vk::ImageMemoryBarrier2> {
        self.dst_info.as_ref().map(|(dst_stage_mask, dst_access_mask, dst_layout)| {
            let mut barrier = vk::ImageMemoryBarrier2::builder()
                .image(self.image.get_handle())
                .subresource_range(self.subresource_range)
                .src_stage_mask(src_stage_mask)
                .src_access_mask(src_access_mask)
                .old_layout(vk::ImageLayout::GENERAL)
                .new_layout(*dst_layout);

            if let Some((src_queue_family, dst_queue_family)) = &self.queue_info {
                barrier = barrier
                    .src_queue_family_index(*src_queue_family)
                    .dst_queue_family_index(*dst_queue_family);
            } else {
                barrier = barrier
                    .dst_stage_mask(*dst_stage_mask)
                    .dst_access_mask(*dst_access_mask);
            }

            barrier.build()
        })
    }
}

pub struct StagingMemory {
    transfer: Arc<Transfer>,
    memory: *mut u8,
    memory_size: usize,
    memory_id: UUID,
    buffer_offset: vk::DeviceSize,
}

impl StagingMemory {
    pub fn get_memory_size(&self) -> usize {
        self.memory_size
    }

    /// Writes the data stored in the slice to the memory and returns the number of bytes written.
    /// If the data does not fit into the available memory range [`None`] is returned.
    ///
    /// # Safety
    /// This function is fully safe from out of bounds memory accesses. However it is the
    /// responsibility of the caller to ensure that no concurrent writes take place on the same
    /// region.
    pub unsafe fn write<T: ToBytes + ?Sized>(&self, data: &T) -> Option<usize> {
        self.write_offset(data, 0)
    }

    /// Writes the data stored in the slice to the memory at the specified offset and returns the
    /// number of bytes written.
    /// If the data does not fit into the available memory range [`None`] is returned.
    ///
    /// # Safety
    /// This function is fully safe from out of bounds memory accesses. However it is the
    /// responsibility of the caller to ensure that no concurrent writes take place on the same
    /// region.
    pub unsafe fn write_offset<T: ToBytes + ?Sized>(&self, data: &T, offset: usize) -> Option<usize> {
        assert!(offset <= (isize::MAX as usize));

        let src = data.as_bytes();
        if offset + src.len() > self.memory_size {
            return None;
        }

        let dst = std::slice::from_raw_parts_mut(self.memory.offset(offset as isize), src.len());
        dst.copy_from_slice(src);

        Some(src.len())
    }

    pub unsafe fn read<T: FromBytes + ?Sized>(&self, data: &mut T) -> Option<usize> {
        self.read_offset(data, 0)
    }

    pub unsafe fn read_offset<T: FromBytes + ?Sized>(&self, data: &mut T, offset: usize) -> Option<usize> {
        assert!(offset <= (isize::MAX as usize));

        let dst = data.as_bytes_mut();
        if offset + dst.len() > self.memory_size {
            return None;
        }

        let src = std::slice::from_raw_parts(self.memory.offset(offset as isize), dst.len());
        dst.copy_from_slice(src);

        Some(dst.len())
    }

    pub unsafe fn copy_to_buffer<T: Into<BufferId>>(&self, dst_buffer: T, mut ranges: BufferTransferRanges) {
        ranges.add_src_offset(self.buffer_offset);
        let task = Task::BufferTransfer(BufferTransfer {
            src_buffer: BufferId::from_raw(self.memory_id),
            dst_buffer: dst_buffer.into(),
            ranges
        });
        self.transfer.share.push_task(task);
    }

    pub unsafe fn copy_from_buffer<T: Into<BufferId>>(&self, src_buffer: T, mut ranges: BufferTransferRanges) {
        ranges.add_dst_offset(self.buffer_offset);
        let task = Task::BufferTransfer(BufferTransfer {
            src_buffer: src_buffer.into(),
            dst_buffer: BufferId::from_raw(self.memory_id),
            ranges
        });
        self.transfer.share.push_task(task);
    }

    pub unsafe fn copy_to_image<T: Into<ImageId>>(&self, dst_image: T, mut ranges: BufferImageTransferRanges) {
        ranges.add_buffer_offset(self.buffer_offset);
        let task = Task::BufferToImageTransfer(BufferToImageTransfer {
            src_buffer: BufferId::from_raw(self.memory_id),
            dst_image: dst_image.into(),
            ranges
        });
        self.transfer.share.push_task(task);
    }

    pub fn flush(&self) {
        todo!()
    }
}

unsafe impl Send for StagingMemory {
}

unsafe impl Sync for StagingMemory {
}

impl Drop for StagingMemory {
    fn drop(&mut self) {
        self.transfer.share.push_task(Task::StagingRelease(self.memory_id));
    }
}

#[derive(Copy, Clone, PartialEq, Eq, Debug)]
pub struct BufferTransferRange {
    pub src_offset: vk::DeviceSize,
    pub dst_offset: vk::DeviceSize,
    pub size: vk::DeviceSize,
}

impl BufferTransferRange {
    pub fn new(src_offset: vk::DeviceSize, dst_offset: vk::DeviceSize, size: vk::DeviceSize) -> Self {
        Self {
            src_offset,
            dst_offset,
            size
        }
    }
}

#[derive(Clone, PartialEq, Eq, Debug)]
pub enum BufferTransferRanges {
    One(BufferTransferRange),
    Multiple(Box<[BufferTransferRange]>),
}

impl BufferTransferRanges {
    pub fn new_single(src_offset: vk::DeviceSize, dst_offset: vk::DeviceSize, size: vk::DeviceSize) -> Self {
        Self::One(BufferTransferRange::new(src_offset, dst_offset, size))
    }

    pub fn add_src_offset(&mut self, src_offset: vk::DeviceSize) {
        match self {
            BufferTransferRanges::One(range) => range.src_offset += src_offset,
            BufferTransferRanges::Multiple(ranges) => {
                for range in ranges.as_mut() {
                    range.src_offset += src_offset;
                }
            }
        }
    }

    pub fn add_dst_offset(&mut self, dst_offset: vk::DeviceSize) {
        match self {
            BufferTransferRanges::One(range) => range.dst_offset += dst_offset,
            BufferTransferRanges::Multiple(ranges) => {
                for range in ranges.as_mut() {
                    range.dst_offset += dst_offset;
                }
            }
        }
    }

    pub fn as_slice(&self) -> &[BufferTransferRange] {
        match self {
            BufferTransferRanges::One(range) => std::slice::from_ref(range),
            BufferTransferRanges::Multiple(ranges) => ranges.as_ref(),
        }
    }
}

#[derive(Clone, Debug)]
pub struct BufferTransfer {
    pub src_buffer: BufferId,
    pub dst_buffer: BufferId,
    pub ranges: BufferTransferRanges,
}

impl BufferTransfer {
    pub fn new_single_range<S: Into<BufferId>, D: Into<BufferId>>(
        src_buffer: S,
        src_offset: vk::DeviceSize,
        dst_buffer: D,
        dst_offset: vk::DeviceSize,
        size: vk::DeviceSize
    ) -> Self {
        Self {
            src_buffer: src_buffer.into(),
            dst_buffer: dst_buffer.into(),
            ranges: BufferTransferRanges::new_single(src_offset, dst_offset, size)
        }
    }
}

#[derive(Copy, Clone, PartialEq, Eq, Debug)]
pub struct BufferImageTransferRange {
    pub buffer_offset: vk::DeviceSize,
    pub buffer_row_length: u32,
    pub buffer_image_height: u32,
    pub image_aspect_mask: vk::ImageAspectFlags,
    pub image_mip_level: u32,
    pub image_base_array_layer: u32,
    pub image_layer_count: u32,
    pub image_offset: Vec3i32,
    pub image_extent: Vec3u32,
}

#[derive(Clone, PartialEq, Eq, Debug)]
pub enum BufferImageTransferRanges {
    One(BufferImageTransferRange),
    Multiple(Box<[BufferImageTransferRange]>),
}

impl BufferImageTransferRanges {
    pub fn as_slice(&self) -> &[BufferImageTransferRange] {
        match self {
            Self::One(range) => std::slice::from_ref(range),
            Self::Multiple(ranges) => ranges.as_ref(),
        }
    }

    pub fn add_buffer_offset(&mut self, offset: vk::DeviceSize) {
        match self {
            BufferImageTransferRanges::One(range) => range.buffer_offset += offset,
            BufferImageTransferRanges::Multiple(ranges) => {
                for range in ranges.as_mut() {
                    range.buffer_offset += offset;
                }
            }
        }
    }
}

#[derive(Clone, Debug)]
pub struct BufferToImageTransfer {
    pub src_buffer: BufferId,
    pub dst_image: ImageId,
    pub ranges: BufferImageTransferRanges,
}

#[derive(Clone, Debug)]
pub struct ImageToBufferTransfer {
    pub src_image: ImageId,
    pub dst_buffer: BufferId,
    pub ranges: BufferImageTransferRanges,
}

#[cfg(test)]
mod tests {
    use crate::vk::objects::allocator::AllocationStrategy;
    use crate::vk::test::make_headless_instance_device;
    use super::*;

    fn create_test_buffer(device: &DeviceEnvironment, size: usize) -> Buffer {
        let info = vk::BufferCreateInfo::builder()
            .size(size as vk::DeviceSize)
            .usage(vk::BufferUsageFlags::TRANSFER_SRC | vk::BufferUsageFlags::TRANSFER_DST)
            .sharing_mode(vk::SharingMode::EXCLUSIVE);

        let buffer = unsafe {
            device.vk().create_buffer(&info, None)
        }.unwrap();

        let allocation = device.get_allocator().allocate_buffer_memory(buffer, &AllocationStrategy::AutoGpuOnly).unwrap();

        unsafe {
            device.vk().bind_buffer_memory(buffer, allocation.memory(), allocation.offset())
        }.unwrap();

        Buffer::new(buffer)
    }

    #[test]
    fn test_buffer_copy() {
        env_logger::init();

        let (_, device) = make_headless_instance_device();

        let buffer = create_test_buffer(&device, 1024);
        let transfer = device.get_transfer();

        let data: Vec<_> = (0u32..16u32).collect();
        let byte_size = data.len() * std::mem::size_of::<u32>();

        let op = transfer.prepare_buffer_acquire(buffer, None);
        transfer.acquire_buffer(op, SemaphoreOps::None).unwrap();

        let mut write_mem = transfer.request_staging_memory(byte_size);
        unsafe {
            write_mem.write(data.as_slice());
            write_mem.copy_to_buffer(buffer, BufferTransferRanges::new_single(0, 0, byte_size as vk::DeviceSize));
        }

        let mut dst_data = Vec::new();
        dst_data.resize(data.len(), 0u32);

        let mut read_mem = transfer.request_staging_memory(byte_size);
        unsafe {
            read_mem.copy_from_buffer(buffer, BufferTransferRanges::new_single(0, 0, byte_size as vk::DeviceSize));
        }

        let op = transfer.prepare_buffer_release(buffer, None);
        let id = transfer.release_buffer(op).unwrap();
        transfer.flush(id);

        transfer.wait_for_complete(id);
        unsafe {
            read_mem.read(dst_data.as_mut_slice()).unwrap();
        }

        unsafe {
            device.vk().destroy_buffer(buffer.get_handle(), None)
        };

        assert_eq!(data, dst_data);
    }
}