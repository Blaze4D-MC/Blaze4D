use std::cmp::Ordering;
use std::collections::hash_map::DefaultHasher;
use std::fmt::{Debug, Formatter};
use std::hash::{Hash, Hasher};
use std::num::NonZeroU64;
use std::sync::Arc;

/// Packed struct providing a 47bit field, a 16 bit field and a always set 1bit niche.
#[derive(Copy, Clone, Ord, PartialOrd, Eq, PartialEq, Hash)]
struct NonZeroU47U16(NonZeroU64);

impl NonZeroU47U16 {
    const NICHE_BIT: u64 = 1u64 << 47u64;

    const LITTLE_OFFSET: u32 = 48u32;
    const LITTLE_MAX: u64 = (1u64 << 16u64) - 1u64;
    const LITTLE_MASK: u64 = Self::LITTLE_MAX << Self::LITTLE_OFFSET;

    const BIG_OFFSET: u64 = 0u64;
    const BIG_MAX: u64 = (1u64 << 47u64) - 1u64;
    const BIG_MASK: u64 = Self::BIG_MAX << Self::BIG_OFFSET;

    const fn new(big: u64, little: u16) -> Self {
        if big > Self::BIG_MAX {
            panic!("Big out of range");
        }

        let shifted_little = (little as u64) << Self::LITTLE_OFFSET;
        let shifted_big = big << Self::BIG_OFFSET;

        unsafe { // Need const unwrap
            Self(NonZeroU64::new_unchecked(shifted_little | shifted_big | Self::NICHE_BIT))
        }
    }

    const fn from_u64(value: u64) -> Self {
        unsafe { // Need const unwrap
            Self(NonZeroU64::new_unchecked(value | Self::NICHE_BIT))
        }
    }

    const fn get_big(&self) -> u64 {
        (self.0.get() & Self::BIG_MASK) >> Self::BIG_OFFSET
    }

    const fn get_little(&self) -> u16 {
        ((self.0.get() & Self::LITTLE_MASK) >> Self::LITTLE_OFFSET) as u16
    }
}

impl Debug for NonZeroU47U16 {
    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
        f.debug_struct("NonZeroU47U16")
            .field("big", &self.get_big())
            .field("little", &self.get_little())
            .finish()
    }
}

/// A global id backed by a 64 bit value.
///
/// Global ids are guaranteed to be globally unique. They are generated by an incrementing u64 bit
/// internal counter and will always be non zero.
///
/// # Examples
///
/// ```
/// use rosella_rs::util::id::GlobalId;
///
/// // Creates a new global id
/// let id = GlobalId::new();
///
/// // This is still the same id
/// let same_id = id.clone();
///
/// assert_eq!(id, same_id);
///
/// // Creates a new different global id
/// let other_id = GlobalId::new();
///
/// assert_ne!(id, other_id);
///
/// // 0 is a niche so Options are free
/// assert_eq!(8, std::mem::size_of::<Option<GlobalId>>());
/// ```
#[derive(Copy, Clone, Eq, PartialEq, Ord, PartialOrd, Hash)]
pub struct GlobalId(NonZeroU64);

impl GlobalId {
    // Need to reserve value 1 for the NamedId address space
    const NEXT_GLOBAL_ID: std::sync::atomic::AtomicU64 = std::sync::atomic::AtomicU64::new(2u64);

    /// Creates a new globally unique id
    ///
    /// # Panics
    ///
    /// This function will panic if the internal 64bit counter overflows.
    pub fn new() -> Self {
        let next = Self::NEXT_GLOBAL_ID.fetch_add(1, std::sync::atomic::Ordering::Relaxed);

        match NonZeroU64::new(next) {
            Some(val) => Self(val),
            None => panic!("GlobalId overflow!")
        }
    }

    /// Creates a global id from a raw 64bit value.
    ///
    /// This value **must** have previously been created by a call to [`GlobalId::new()`] otherwise
    /// this **will** result in undefined behaviour.
    ///
    /// # Panics
    ///
    /// The function will panic if the id is `0`.
    pub const fn from_raw(id: u64) -> Self {
        if id == 0u64 {
            panic!("Id must not be 0");
        }

        unsafe { // Need const unwrap
            Self(NonZeroU64::new_unchecked(id))
        }
    }

    /// Returns the raw 64bit global id.
    pub fn get_raw(&self) -> u64 {
        self.0.get()
    }
}

impl Into<u64> for GlobalId {
    fn into(self) -> u64 {
        self.get_raw()
    }
}

impl Debug for GlobalId {
    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
        f.debug_tuple("GlobalId").field(&self.get_raw()).finish()
    }
}

/// A local id backed by a 47bit value and a 16bit value.
///
/// While global ids are guaranteed to be globally unique, local ids must not be and can be
/// generated in any way. The pair of a global id and a local id creates a globally unique
/// identifier.
///
/// Local ids are split into a 47bit field called big and a 16bit field called little. The remaining
/// bit is a always set niche bit. An extra utility function is provided to create a local id from
/// a 64bit hash value.
#[derive(Copy, Clone, Eq, PartialEq, Ord, PartialOrd, Hash)]
pub struct LocalId(NonZeroU47U16);

impl LocalId {
    pub const BIG_MAX: u64 = NonZeroU47U16::BIG_MAX;

    /// Creates a local id form a big little pair.
    pub const fn new(big: u64, little: u16) -> Self {
        Self(NonZeroU47U16::new(big, little))
    }

    /// Creates a local id from a hash value. One of the bits in the value will be overwritten for
    /// the niche bit.
    pub const fn from_hash(hash: u64) -> Self {
        Self(NonZeroU47U16::from_u64(hash))
    }

    pub fn get_big(&self) -> u64 {
        self.0.get_big()
    }

    pub fn get_little(&self) -> u16 {
        self.0.get_little()
    }
}

impl Debug for LocalId {
    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
        f.debug_struct("LocalId")
            .field("big", &self.get_big())
            .field("little", &self.get_little())
            .finish()
    }
}

/// A universally unique identified.
///
/// A uuid is made up of a global, local id pair.
#[derive(Copy, Clone, Eq, PartialEq, Ord, PartialOrd, Hash, Debug)]
pub struct UUID {
    pub global: GlobalId,
    pub local: LocalId,
}

/// A utility struct providing a simple incrementing counter local id generator.
pub struct IncrementingGenerator {
    global: GlobalId,
    next: std::sync::atomic::AtomicU64,
}

impl IncrementingGenerator {
    pub fn new() -> Self {
        Self {
            global: GlobalId::new(),
            next: std::sync::atomic::AtomicU64::new(0),
        }
    }

    pub fn get_global_id(&self) -> GlobalId {
        self.global
    }

    pub fn next(&self) -> Option<UUID> {
        let local = self.next.fetch_add(1u64, std::sync::atomic::Ordering::Relaxed);

        if local > LocalId::BIG_MAX {
            return None;
        }

        Some(UUID {
            global: self.global,
            local: LocalId::new(local, 0u16),
        })
    }

    pub fn next_with_little(&self, little: u16) -> Option<UUID> {
        let local = self.next.fetch_add(1u64, std::sync::atomic::Ordering::Relaxed);

        if local > LocalId::BIG_MAX {
            return None;
        }

        Some(UUID {
            global: self.global,
            local: LocalId::new(local, little)
        })
    }
}

/// Quickly identify and compare entities while retaining a human readable name.
///
/// comparing existing ID's is very fast so it is highly
/// recommended to avoid creating new instances when not necessary. (Also reduces typing mistakes)
#[derive(Clone, Debug)]
pub struct NamedUUID {
    name: Arc<String>,
    id: LocalId,
}

impl NamedUUID {
    /// The global id used by all NamedUUIDs
    pub const GLOBAL_ID: GlobalId = GlobalId::from_raw(1u64);

    pub fn new(name: String) -> NamedUUID {
        let mut hasher = DefaultHasher::new();
        name.hash(&mut hasher);
        let hash = hasher.finish();

        NamedUUID { name: Arc::new(name), id: LocalId::from_hash(hash) }
    }

    pub fn get_name(&self) -> &String {
        self.name.as_ref()
    }

    pub fn get_uuid(&self) -> UUID {
        UUID {
            global: Self::GLOBAL_ID,
            local: self.id,
        }
    }

    pub fn get_global_id(&self) -> GlobalId {
        Self::GLOBAL_ID
    }

    pub fn get_local_id(&self) -> LocalId {
        self.id
    }
}
impl PartialEq for NamedUUID {
    fn eq(&self, other: &Self) -> bool {
        self.id.eq(&other.id)
    }
}

impl Eq for NamedUUID {
}

impl PartialEq<UUID> for NamedUUID {
    fn eq(&self, other: &UUID) -> bool {
        self.get_uuid().eq(other)
    }
}

impl PartialOrd for NamedUUID {
    fn partial_cmp(&self, other: &Self) -> Option<Ordering> {
        self.id.partial_cmp(&other.id)
    }
}

impl Ord for NamedUUID {
    fn cmp(&self, other: &Self) -> Ordering {
        self.id.cmp(&other.id)
    }
}

impl PartialOrd<UUID> for NamedUUID {
    fn partial_cmp(&self, other: &UUID) -> Option<Ordering> {
        self.get_uuid().partial_cmp(other)
    }
}

impl Hash for NamedUUID {
    fn hash<H: Hasher>(&self, state: &mut H) {
        // The hash should be identical to the one generated from the uuid
        self.get_uuid().hash(state)
    }
}